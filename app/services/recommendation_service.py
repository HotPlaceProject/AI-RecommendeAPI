# app/services/recommendation_service.py
import json
from typing import Optional
import os

from langchain.chains.llm import LLMChain
from langchain_core.messages import BaseMessage
# LangChain, OpenAI ê´€ë ¨ ì„í¬íŠ¸
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.runnables import RunnableConfig, chain, RunnableSerializable
from langchain_openai import ChatOpenAI
from typing import Any, List

# Pydantic ëª¨ë¸
from app.api.vi.schemas.recommendation import Recommendation, User, RecommendationRequest

# config.pyì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œ
from app.core.config import settings
os.environ["OPENAI_API_KEY"] = settings.OPENAI_API_KEY
os.environ["TAVILY_API_KEY"] = settings.TAVILY_API_KEY

# ê²€ìƒ‰ íˆ´
from app.langchain_tools.search_web import search_web


def create_prompt_template(feature: str) -> ChatPromptTemplate:
    """
    ChatPromptTemplateì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    """
    prompt_template = ChatPromptTemplate([
        ("system", f"""
          ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ë‚˜ì´, ì„±ë³„, í¬ë§ ì¹´í…Œê³ ë¦¬ì„ ê¸°ë°˜ìœ¼ë¡œ ë§ì¶¤í˜• ì‹ë‹¹ì„ ì¶”ì²œí•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

          ì•„ë˜ ì‚¬ìš©ìì˜ íŠ¹ì„±ì„ ë°˜ì˜í•´ì„œ ë§ì¶¤í˜• ë§›ì§‘ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”
          {feature}

        """),
        ("user", "#Format: {format_instructions}\n\n#user_input: {user_input}"),
        ("placeholder", "{messages}")
    ])
    output_parser = JsonOutputParser(pydantic_object=Recommendation) # output parser JsonOutputParserë¡œ ì„¤ì •
    prompt = prompt_template.partial(format_instructions=output_parser.get_format_instructions())

    return prompt


def initialize_llm() -> ChatOpenAI:
    """
    LLMì„ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    return ChatOpenAI(model="gpt-4o-mini")


def execute_web_search_chain(prompt: ChatPromptTemplate) -> Any:
    """
    LLMê³¼ ì›¹ ê²€ìƒ‰ íˆ´ì„ ì—°ê²°í•œ ì²´ì¸ì„ ìƒì„±í•˜ì—¬ ë°˜í™˜
    """
    llm = initialize_llm()
    llm_with_tools = llm.bind_tools(tools=[search_web])
    llm_chain = prompt | llm_with_tools

    return llm_chain  # ğŸ”¹ ì²´ì¸ë§Œ ë°˜í™˜ (invoke ì‹¤í–‰ X)


def recommend_restaurant(req: RecommendationRequest) -> dict:
    """
    ì¶”ì²œ ìš”ì²­ì„ ë°›ì•„ì„œ ë§›ì§‘ì„ ì¶”ì²œí•˜ëŠ” í•¨ìˆ˜
    """
    feature = f"ë‚˜ì´: {req.user.age}, ì„±ë³„: {req.user.gender}, ì¹´í…Œê³ ë¦¬: {req.category}"
    prompt = create_prompt_template(feature)

    # ì²´ì¸ ìƒì„±
    llm_with_chain = execute_web_search_chain(prompt)

    # invoke ì‹¤í–‰ (1ì°¨ í˜¸ì¶œ -> Tool í˜¸ì¶œ)
    ai_msg = llm_with_chain.invoke({"user_input": req.region}, config=RunnableConfig())
    print("Tool Calling:", ai_msg)

    # search_web í˜¸ì¶œí•˜ì—¬ ì¶”ê°€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    tool_msgs = search_web.batch(ai_msg.tool_calls, config=RunnableConfig())

    # LLMì—ê²Œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì „ë‹¬í•˜ì—¬ ìµœì¢… ì‘ë‹µ ìƒì„± (2ì°¨ í˜¸ì¶œ)
    final_response = llm_with_chain.invoke({"user_input": req.region, "messages": [ai_msg, *tool_msgs]}, config=RunnableConfig())
    print("Final AI Response:", final_response)

    # JSON íŒŒì‹±
    json_str = final_response.content.replace("```json\n", "").replace("\n```", "")
    return json.loads(json_str, strict=False)



